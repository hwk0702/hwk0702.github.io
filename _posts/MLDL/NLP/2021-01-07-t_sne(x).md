---

layout: post

title: "t-SNE"

date: 2021-01-07 19:30:07

categories: [Machine Learning]

description:

image: /img/tsne.png

published: False

canonical_url:

tags: [Machine Learning, Dimensionality Reduction, Stochastic Neighbor Embedding, SNE, t-SNE, 머신러닝, 차원축소]

---

> 이 내용은 고려대학교 강필성 교수님의 Business Analytics 수업을 보고 정리한 것입니다.
>
> 아래 이미지 클릭 시 강의 영상 Youtube URL로 넘어갑니다.
>
>[![01-7: Dimensionality Reduction - tSNE](https://img.youtube.com/vi/INHwh8k4XhM/mqdefault.jpg)](https://youtu.be/INHwh8k4XhM)

## Stochastic Neighbor Embedding

- 로컬이 아닌 거리보다 로컬 거리를 올바르게 얻는 것이 더 중요
- SNE는 쌍방향 거리가 로컬인지 여부를 결정하는 확률론적 방법을 이용한다.(but 가까운 이웃일수록 선택된 확률이 높다.)
- 각 고차원 유사성을 한 데이터 포인트가 다른 데이터 포인트를 이웃으로 선택할 확률로 변환
  - Probability of picking j given in <span style="color:blue">high D </span>
  - Probability of picking j given in <span style="color:red">low D </span>

$$\color{blue}p_{j|i}\color{black}=\frac{e^{-\frac{||\color{blue}\mathbf{x}_i\color{black}-\color{blue}\mathbf{x}_j\color{black}||^2}{2\sigma^2_i}}}{\sum_{k\neq i}e^{-\frac{||\color{blue}\mathbf{x}_i\color{black}-\color{blue}\mathbf{x}_k\color{black}||^2}{2\sigma^2_i}}}\;\;\;\;\;\;\;\;\;\;\color{red}q_{j|i}\color{black}=\frac{e^{-\frac{||\color{red}\mathbf{y}_i\color{black}-\color{red}\mathbf{y}_j\color{black}||^2}{2\sigma^2_i}}}{\sum_{k\neq i}e^{-\frac{||\color{red}\mathbf{y}_i\color{black}-\color{red}\mathbf{y}_k\color{black}||^2}{2\sigma^2_i}}}$$

$$\color{blue}p_{j|i}$$: 원래 차원($$d$$)에서 객체 $$i$$가 $$j$$를 이웃으로 선택할 확률

$$\color{red}q_{j|i}$$: 축소된 차원($$d'$$)에서 객체 $$i$$가 $$j$$를 이웃으로 선택할 확률

### p에서 가우스 반경 선택하기

- 공간의 다른 부분에서 다른 반지름을 사용하여 유효 이웃 수를 일정하게 유지
- 반경이 크면 이웃 i에 대한 분포에 대해 높은 엔트로피가 발생하는 반면, 반경이 작으면 낮은 엔트로피
- 원하는 엔트로피를 결정한 다음 해당 엔트로피를 생성하는 반경을 찾는다.

$$\begin{aligned}
Perplexity(P_i)=2^{H(P_i)}\\
H(P_i)=\sum_j\color{blue}P_{j|i}\color{black}log_2\color{blue}P_{j|i}
\end{aligned}\;\;\;\;\;\;\;\color{blue}p_{j|i}\color{black}=\frac{e^{-\frac{||\color{blue}\mathbf{x}_i\color{black}-\color{blue}\mathbf{x}_j\color{black}||^2}{2\sigma^2_i}}}{\sum_{k\neq i}e^{-\frac{||\color{blue}\mathbf{x}_i\color{black}-\color{blue}\mathbf{x}_k\color{black}||^2}{2\sigma^2_i}}}$$

- SNE의 성능은 preplexity의 변화에 ​​상당히 견고 (5~50)

### 저 차원 표현을위한 비용 함수

- Kullback-Leibler divergence (두 분포가 얼마나 유사한지)
  - 두 확률 분포 P와 Q의 차이에 대한 비대칭 측도

$$Cost = \sum_iKL(\color{blue}P_i\color{black}||\color{red}Q_i\color{black})=\sum_i\sum_j\color{blue}p_{j|i}\color{black}log\frac{\color{blue}p_{j|i}}{\color{red}q_{j|i}}$$

0에 가까울수록 두 분포 비슷, 크면 클수록 두 분포가 다름

<img src='/img/tsne01.png' width='600'>

- Gradient
  - $$\mathbf{y}_k$$는 식의 정규화 된 항을 통해 $$q_{ij}$$에 영향을 미치기 때문에 미분 비용은 장황하지만 결과는 간단하다.
  - 그라디언트는 매우 단순한 형태

$$\frac{\partial C}{\partial \color{red}\mathbf{y}_i}=2\sum_j(\color{red}\mathbf{y}_j\color{black}-\color{red}\mathbf{y}_i\color{black})(\color{blue}p_{j|i}\color{black}-\color{red}q_{j|i}\color{black}+\color{blue}p_{j|i}\color{black}-\color{red}q_{j|i}\color{black})$$

- Gradient of the cost function

$$\begin{aligned}
 C&=\sum_iKL(\color{blue}P_i\color{black}||\color{red}Q_i\color{black})=\sum_i\sum_j\color{blue}p_{j|i}\color{black}log\frac{\color{blue}p_{j|i}}{\color{red}q_{j|i}} \\
 &= \sum_i\sum_j\color{blue}p_{j|i}\color{black}log{\color{blue}p_{j|i}}-\sum_i\sum_j\color{blue}p_{j|i}\color{black}log{\color{red}q_{j|i}}
\end{aligned}$$

$$\begin{aligned}
C'&=-\sum_i\sum_j\color{blue}p_{j|i}\color{black}log{\color{red}q_{j|i}} \;\;\;\;\;\;(\frac{\partial C}{\partial \mathbf{y}_t}=\frac{\partial C'}{\partial \mathbf{y}_t})\\
&=-\sum_i\color{blue}p_{t|i}\color{black}log{\color{red}q_{t|i}}-\sum_j\color{blue}p_{j|t}\color{black}log{\color{red}q_{j|t}}-\sum_{i\neq t}\sum_{j \neq t}\color{blue}p_{i|j}\color{black}log{\color{red}q_{i|j}}
\end{aligned}$$

$$d_{ti}=exp(-||\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_i\color{black}||^2)=d_{it}$$

$$\frac{\partial d_{ti}}{\partial\color{red}\mathbf{y}_t}=d'_{ti}=-2(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_i\color{black})exp(-||\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_i\color{black}||^2)=-2(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_i\color{black})d_{ti}$$

$$\color{red}q_{t|i}\color{black}=\frac{exp(-||\color{red}\mathbf{y}_i\color{black}-\color{red}\mathbf{y}_t\color{black}||^2)}{\sum_{k\neq i}exp(-||\color{red}\mathbf{y}_i\color{black}-\color{red}\mathbf{y}_k\color{black}||^2)}=\frac{d_{it}}{\sum_{k \neq i}d_{ik}}$$

$$\color{red}q_{j|t}\color{black}=\frac{exp(-||\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black}||^2)}{\sum_{k\neq t}exp(-||\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_k\color{black}||^2)}=\frac{d_{tj}}{\sum_{k \neq t}d_{tk}}$$

$$\color{red}q_{i|j}\color{black}=\frac{exp(-||\color{red}\mathbf{y}_j\color{black}-\color{red}\mathbf{y}_i\color{black}||^2)}{\sum_{k\neq j}exp(-||\color{red}\mathbf{y}_j\color{black}-\color{red}\mathbf{y}_k\color{black}||^2)}=\frac{d_{ji}}{\sum_{k \neq j}d_{jk}}$$

(1) $$-\sum_i\color{blue}p_{t|i}\color{black}log{\color{red}q_{t|i}}$$을 $$\mathbf{y}_t$$에 대해서 미분한 값을 구하게 되면

$$\begin{aligned}
\frac{\partial}{\partial \mathbf{y}_t}(-\sum_i \color{blue}p_{t|i}\color{black}log\color{red}q_{t|i}\color{black})\color{black}&=-\sum_i \color{blue}p_{t|i}\color{black}\cdot\frac{1}{\color{red}q_{t|i}}\cdot\frac{\partial \color{red}q_{t|i}}{\partial \mathbf{y}_t} \\
&=-\sum_i\color{blue}p_{t|i}\color{black}\cdot\frac{1}{\color{red}q_{t|i}}\cdot\frac{d'_{it}\cdot(\sum_{k \neq i}d_{ik})-d_{it}\cdot d'_{it}}{(\sum_{k \neq i}d_{ik})^2} \\
&= -\sum_i\color{blue}p_{t|i}\color{black}\cdot\frac{1}{\color{red}q_{t|i}}\cdot\frac{-2(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_i\color{black})\cdot d_{it}\cdot(\sum_{k \neq i}d_{ik})+2(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_i\color{black})\cdot d_{it}^2)}{(\sum_{k \neq i}d_{ik})^2} \\
&=-\sum_i\color{blue}p_{t|i}\color{black}\cdot\frac{1}{\color{red}q_{t|i}}\cdot(-2(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_i\color{black})\cdot \color{red}q_{t|i}\color{black}+2(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_i\color{black})\cdot\color{red}q_{it}^2\color{balck}) \\
&=\sum_i\color{blue}p_{t|i}\color{black}\cdot 2(\color{red}\mathbf{y}_t\color{balck}-\color{red}\mathbf{y}_i\color{balck})(1-\color{red}q_{t|i}\color{black})
\end{aligned}$$

(2) $$-\sum_j\color{blue}p_{j|t}\color{black}log{\color{red}q_{j|t}}$$을 $$\mathbf{y}_t$$에 대해서 미분한 값을 구하게 되면

$$\begin{aligned}
\frac{\partial}{\partial \mathbf{y}_t}(-\sum_j \color{blue}p_{j|t}\color{black}log\color{red}q_{j|t}\color{black})\color{black}&=-\sum_j \color{blue}p_{j|t}\color{black}\cdot\frac{1}{\color{red}q_{j|t}}\cdot\frac{\partial \color{red}q_{j|t}}{\partial \mathbf{y}_t} \\
&=-\sum_j\color{blue}p_{j|t}\color{black}\cdot\frac{1}{\color{red}q_{j|t}}\cdot\frac{d'_{tj}\cdot(\sum_{k \neq t}d_{tk})-d_{tj}\cdot(\sum_{k \neq t}d'_{tk})}{(\sum_{k \neq t}d_{tk})^2} \\
&= -\sum_j\color{blue}p_{j|t}\color{black}\cdot\frac{1}{\color{red}q_{j|t}}\cdot\frac{-2(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})\cdot d_{tj}\cdot(\sum_{k \neq t}d_{tk})-d_{tj}\cdot(\sum_{k \neq t}d'_{tk})}{(\sum_{k \neq t}d_{tk})^2} \\
&=2\sum_j\color{blue}p_{j|t}\color{black}\cdot(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})+\sum_j \color{blue}p_{j|t}\color{black}\cdot\frac{\sum_{k \neq t}d'_{tk}}{\sum_{k \neq t}d_{tk}} \\
&=2\sum_j\color{blue}p_{j|t}\color{black}\cdot(\color{red}\mathbf{y}_t\color{balck}-\color{red}\mathbf{y}_j\color{balck})+\sum_j \cdot \frac{d'_{tj}}{\sum_{k \neq t}d_{tk}}\\
&=2\sum_j\color{blue}p_{j|t}\color{black}\cdot(\color{red}\mathbf{y}_t\color{balck}-\color{red}\mathbf{y}_j\color{balck})-2\sum_{j}(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})\cdot\frac{d_{tj}}{\sum_{k \neq t}d_{tk}}\\
&=2\sum_j\color{blue}p_{j|t}\color{black}\cdot(\color{red}\mathbf{y}_t\color{balck}-\color{red}\mathbf{y}_j\color{balck})-2\sum_{j}(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})\cdot\color{red}q_{j|t}\color{black}\\
&=2\sum_j
(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})(\color{blue}p_{j|t}\color{black}-\color{red}q_{j|t}\color{black})\end{aligned}$$

(3) $$-\sum_{i \neq t}\sum_{j \neq t}\color{blue}p_{i|j}\color{black}log{\color{red}q_{i|j}}$$을 $$\mathbf{y}_t$$에 대해서 미분한 값을 구하게 되면

$$\begin{aligned}
\frac{\partial}{\partial \mathbf{y}_t}(-\sum_{i \neq t}\sum_{j \neq t} \color{blue}p_{i|j}\color{black}log\color{red}q_{i|j}\color{black})\color{black}&=-\sum_{i \neq t}\sum_{j \neq t} \color{blue}p_{i|j}\color{black}\cdot\frac{1}{\color{red}q_{i|j}}\cdot\frac{\partial \color{red}q_{i|j}}{\partial \mathbf{y}_t} \\
&=-\sum_{i \neq t}\sum_{j \neq t}\color{blue}p_{i|j}\color{black}\cdot\frac{1}{\color{red}q_{i|j}}\cdot\frac{d'_{ji}\cdot\sum_{k \neq j}d_{jk}-d_{ji}\cdot d'_{jt}}{(\sum_{k \neq j}d_{jk})^2} \\
&= -\sum_{i \neq t}\sum_{j \neq t}\color{blue}p_{i|j}\color{black}\cdot\frac{1}{\color{red}q_{i|j}}\cdot\frac{2(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})\cdot d_{ji}\cdot d_{jt}}{(\sum_{k \neq j}d_{jk})^2} \\
&=-\sum_{i \neq t}\sum_{j \neq t}\color{blue}p_{i|j}\color{black}\cdot\frac{1}{\color{red}q_{i|j}}\cdot2(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})\cdot \color{red}q_{i|j}\color{black}\cdot\color{red}q_{t|j} \\
&=-\sum_{i \neq t}\sum_{j \neq t}2(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})\cdot\color{blue}p_{i|j}\color{black}\cdot\color{red}q_{t|j}
\end{aligned}$$

(1)과 (3)을 더하면

$$\sum_i\color{blue}p_{t|i}\color{black}\cdot 2(\color{red}\mathbf{y}_t\color{balck}-\color{red}\mathbf{y}_i\color{balck})(1-\color{red}q_{t|i}\color{black})-\sum_{i \neq t}\sum_{j \neq t}2(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})\cdot\color{blue}p_{i|j}\color{black}\cdot\color{red}q_{t|j}$$

여기서 앞에 $$\sum_i\color{blue}p_{t|i}\color{black}\cdot 2(\color{red}\mathbf{y}_t\color{balck}-\color{red}\mathbf{y}_i\color{balck})(1-\color{red}q_{t|i}\color{black})$$에서 i는 j로 치환될 수 있다.

$$\begin{aligned}
&=2\sum_j(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})\cdot\color{blue}p_{t|j}\color{black}-2\sum_j(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})\cdot\color{blue}p_{t|j}\cdot\color{red}q_{t|j}\color{black}-2\sum_{i \neq t} \sum_{j \neq t}(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{\mathbf{y}}_j\color{black})\cdot\color{blue}p_{i|j}\color{black}\cdot\color{red}q_{t|j}\\
&=2\sum_j(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})\cdot\color{blue}p_{t|j}\color{black}-2\sum_i\sum_j(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})\cdot\color{blue}p_{i|j}\color{black}\cdot\color{red}q_{t|j}\\
&=2\sum_j(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})\cdot\color{blue}p_{t|j}\color{black}-2\sum_j\sum_i\color{blue}p_{i|j}\color{black}\cdot (\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})\cdot\color{red}q_{t|j}\color{black} \;\;\;\; \because (\sum_i\color{blue}p_{i|j}\color{black}=1)\\
&=2\sum_j(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})\cdot\color{blue}p_{t|j}\color{black}-2\sum_j(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})\cdot\color{red}q_{t|j}\color{black}=2\sum_j(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})(\color{blue}p_{t|j}\color{black}-\color{red}q_{t|j}\color{black})
\end{aligned}$$

(1) + (3) 구한 값에 (2)를 더하면

$$\begin{aligned}
2\sum_j(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})(\color{blue}p_{t|j}\color{black}-\color{red}q_{t|j}\color{black})+2\sum_j
(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})(\color{blue}p_{j|t}\color{black}-\color{red}q_{j|t}\color{black}) \\
=2\sum_j(\color{red}\mathbf{y}_t\color{black}-\color{red}\mathbf{y}_j\color{black})(\color{blue}p_{t|j}\color{black}-\color{red}q_{t|j}\color{black}+\color{blue}p_{j|t}\color{black}-\color{red}q_{j|t}\color{black})
\end{aligned}$$

- 비용 함수를 최소화하기 위해 더 낮은 차원의 좌표를 업데이트
  - Gradient update with a momentum term

$$\mathcal{y}^{(t+1)}=\mathcal{y}^{(t)}+\eta\frac{\partial C}{\partial \mathcal{y}}+\alpha (t)(\mathcal{y}^{(t)}-\mathcal{y}^{(t-1)})$$

- 조건부 확률을 쌍별 확률로 바꾸기

$$\color{blue}p_{ij}\color{black}=\frac{e^{-\frac{||\color{blue}\mathbf{X}_i\color{black}-\color{blue}\mathbf{X}_j\color{black}||^2}{2\sigma^2_i}}}{\sum_{k \neq l}e^{-\frac{||\color{blue}\mathbf{X}_k\color{black}-\color{blue}\mathbf{X}_l\color{black}||^2}{2\sigma^2_i}}} \Rightarrow \color{blue}p_{ij}\color{black}=\frac{\color{blue}p_{j|i}\color{black}+\color{blue}p_{i|j}}{2n}\;\;\;\sum_j\color{blule}p_{ij}>\frac{1}{2n}$$

  - cost frunction and gradient

$$Cost=\sum_iKL(\color{blue}P_i\color{black}||\color{red}Q_I\color{black})=\sum_i\sum_j\color{blue}p_{ij}\color{black}log\frac{\color{blue}p_{ij}}{\color{red}q_{ij}}$$

$$\frac{\partial C}{\partial\color{red}\mathbf{y}_i}=4\sum_j(\color{red}\mathbf{y}_j\color{black}-\color{red}\mathbf{y}_i\color{black})(\color{blue}p_{ij}\color{black}-\color{red}q_{ij}\color{black})$$

- Crowding problem
  - 비교적 먼 데이터 포인트를 수용하는 영역이 근처 데이터 포인트를 수용하는 영역에 비해 충분히 크지 않다.

<img src='/img/tsne02.png' width='400'>

---------------------------------------------

## t-SNE

### Resolution to the Crowding problem

- 가우시안보다 꼬리가 두터운 확률 분포를 사용하여 저차원지도에서 거리를 확률로 변환
- 자유도가 1 인 Student's t 분포

$$f(t)=\frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}(1+\frac{t^2}{\nu})^{-\frac{\nu+1}{2}}$$

$$\Gamma(n)=(n-1)!$$

$$\color{red}q_{j|i}\color{black}=\frac{(1+||\color{red}\mathbf{y}_i\color{black}-\color{red}\mathbf{y}_j\color{black}||^2)^{-1}}{\sum_{k \neq l}(1+||\color{red}\mathbf{y}_k\color{black}-\color{red}\mathbf{y}_l\color{black}||^2)^{-1}}$$

<img src='/img/tsne03.png' width='600'>

### Optimization of t-SNE

$$\color{blue}p_{ij}\color{black}=\frac{e^{-\frac{||\color{blue}\mathbf{X}_i\color{black}-\color{blue}\mathbf{X}_j\color{black}||^2}{2\sigma^2_i}}}{\sum_{k \neq l}e^{-\frac{||\color{blue}\mathbf{X}_k\color{black}-\color{blue}\mathbf{X}_l\color{black}||^2}{2\sigma^2_i}}} \;\;\;\;\; \color{red}q_{j|i}\color{black}=\frac{(1+||\color{red}\mathbf{y}_i\color{black}-\color{red}\mathbf{y}_j\color{black}||^2)^{-1}}{\sum_{k \neq l}(1+||\color{red}\mathbf{y}_k\color{black}-\color{red}\mathbf{y}_l\color{black}||^2)^{-1}}$$

- Gradient:

$$\frac{\partial C}{\partial \color{red}\mathbf{y}_i}=4\sum_j(\color{red}\mathbf{y}_i\color{black}-\color{red}\mathbf{y}_j\color{black})(\color{blue}p_{ij}\color{black}-\color{red}q_{ij}\color{black})(1+||\color{red}\mathbf{y}_i\color{black}-\color{red}\mathbf{y}_j\color{black}||^2)^{-1}$$

### t-SNE algorithm

<img src='/img/tsne04.png' width='600'>

---------------------------------------------

## t-SNE Example

- Mnist dataset

<img src='/img/tsne05.png' width='600'>

<img src='/img/tsne06.png' width='600'>

<img src='/img/tsne07.png' width='600'>

- Olivetti faces datasets

<img src='/img/tsne08.png' width='600'>

- Netflix dataset

<img src='/img/tsne09.png' width='600'>
