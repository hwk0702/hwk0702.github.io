---

layout: post

title: "MDS"

date: 2020-12-31 23:30:07

categories: [ML/DL/Machine Learning]

description:

image: /img/mds.png

published: False

canonical_url:

tags: [Machine Learning, Dimensionality Reduction, Supervised Variable Selection, Exhaustive Search, Forward Selection, Backward Elimination, Stepwise Selection, 머신러닝, 차원축소]

---

> 이 내용은 고려대학교 강필성 교수님의 Business Analytics 수업을 보고 정리한 것입니다.
>
> 아래 이미지 클릭 시 강의 영상 Youtube URL로 넘어갑니다.
>
>[![01-5: Dimensionality Reduction - MDS](https://img.youtube.com/vi/Yv00AT4pLC4/mqdefault.jpg)](https://youtu.be/Yv00AT4pLC4)

## Multidimensional Scaling

- 객체들의 거리를 저차원 공간 상에서도 최대한 보존하는 것

<img src='/img/mds01.png' width='600'>

### PCA와 MDS 차이

||PCA|MDS|
|---|---|---|
|Data|d차원 공간의 n개 객체 <br/> ($$\textbf{X}$$ in $$R^d$$)|n 객체 사이의 근접 행렬 <br/> (n by n matrix $$\textbf{D}$$)|
|Purpose|원래 분산을 최대한 유지하기 위한 기저셋 찾기|물체 간의 거리 정보를 유지하는 좌표 세트 찾기|
|Output|1. d bases (eigenvectors, PCs) <br/> 2. d eigenvalues|d 차원에서 각 개체의 좌표 <br/> ($$\textbf{X}$$ in $$R^d$$)|

-------------------------------------------------

### Step 1: Construct Proximity/Distance matrix

- 객체에 대한 좌표가 있으면 객체 간의 유사성/거리를 계산

$$distance-like \; \text{if} \; (1)\;d_{ij} \geq 0, \;(2)\;d_{ii}=0, \;(3)\; d_{ij}=d_{ji}$$

$$metric\;\text{if in addition to (1), (2), (3), it satisfies }d_{ij}\leq d_{ik}+d_{jk}$$

- Distance: Euclidean, Manhattan, etc.
- Similarity: Correlation, Jaccard, etc.
[Distance, Similarity 보러가기](https://hwk0702.github.io/data%20science/data%20mining/2020/01/14/Data-similarity/)

<img src='/img/mds02.png' width='600'>

### Step 2: Extract the coordinates that preserve the distance information

- 거리 행렬 D의 각 요소는 다음과 같이 표현할 수 있다.

$$d^2_{rs}=(\mathbf{x}_r-\mathbf{x}_s)^T(\mathbf{x}_r-\mathbf{x}_s)$$

- 내적 행렬 B는 거리 행렬 D에서 얻을 수 있다.

$$[\mathbf{B}]_{rs}=b_{rs}=\mathbf{x}^T_r\mathbf{x}_s$$

모든 p 변수의 평균이 0이라고 가정하면

$$\sum^n_{r=1}x_{ri}=0, (i=1,2,\cdots,p) \;\;\;\;\;\;\;\;\; d^2_{rs}=\mathbf{x}^T_r\mathbf{x}_r+\mathbf{x}^T_s\mathbf{x}_s-2\mathbf{x}^T_r\mathbf{x}_s$$
