---

layout: post

title: "PCA"

date: 2020-12-27 23:50:07

categories: [ML/DL/Machine Learning]

description:

image: /img/pca01.png

published: False

canonical_url:

tags: [Machine Learning, Dimensionality Reduction, PCA, Principal Component Analysis, 머신러닝, 차원축소, 주성분 분석]

---

> 이 내용은 고려대학교 강필성 교수님의 Business Analytics 수업을 보고 정리한 것입니다.
>
> 아래 이미지 클릭 시 강의 영상 Youtube URL로 넘어갑니다.
>
>[![01-2: Dimensionality Reduction - Supervised Selection](https://i.ytimg.com/vi/bEX6WPMiLvo/hqdefault.jpg?sqp=-oaymwEYCKgBEF5IVfKriqkDCwgBFQAAiEIYAXAB&rs=AOn4CLAxpO91xc-ipMKtSdSTM_BvVEThfQ)](https://youtu.be/yUW8yg4_j6w)

## Principal Component Analysis: PCA

- 원데이터의 분산을 최대한 보존하는 직교하는 기저를 찾는것
- 데이터가 가지고 있는 특정한 속성을 최대한 보존하는 방향으로 새로운 성분을 만들어내는 것

<img src='/img/pca02.PNG' width='600'>

### 목적

- 기준에 따라 예측 후 최대한 분산을 보존 할 수있는 기준 집합을 찾는다.
  - $$X_1, X_2, \cdots, X_p$$: Original variables
  - $$\mathbf{a}_i=[a_{i1}, a_{i2}, \cdots, a_{ip}]$$: $$i^{th}$$ basis or principal component
  - $$Y_1, Y_2, \cdots, Y_p$$: variables after the projection onto the $$i^{th}$$ basis

$$Y_1=\mathbf{a}_1'\mathbf{X}=a_{11}X_1+a_{12}X_2+\cdots+a_{1p}X_p$$

$$Y_2=\mathbf{a}_2'\mathbf{X}=a_{21}X_1+a_{22}X_2+\cdots+a_{2p}X_p$$

$$\vdots$$

$$Y_p=\mathbf{a}_p'\mathbf{X}=a_{p1}X_1+a_{p2}X_2+\cdots+a_{pp}X_p$$

<img src='/img/pca03.PNG' width='600'>

### Covariance

- $$\mathbf{X}$$: a data set (d by n, d: # of variables, n: # of records, 여기서 벡터는 column-wise vector)

$$Cov(\mathbf{X})=\frac{1}{n}(\mathbf{X}-\bar{\mathbf{X}})(\mathbf{X}-\bar{\mathbf{X}})^T$$

- $$Cov(\mathbf{X})_{ij}=Cov(\mathbf{X})_{ji}$$
- Total variance of the data set: $$tr[Cov(\mathbf{X})]=Cov(\mathbf{X})_{11}+Cov(\mathbf{X})_{22}+\cdots+Cov(\mathbf{X})_{dd}$$

<img src='/img/pca04.PNG' width='600'>
