---

layout: post

title: "PCA"

date: 2020-12-29 19:10:07

categories: [ML/DL/Machine Learning]

description:

image: /img/pca01.png

published: True

canonical_url:

tags: [Machine Learning, Dimensionality Reduction, PCA, Principal Component Analysis, ë¨¸ì‹ ëŸ¬ë‹, ì°¨ì›ì¶•ì†Œ, ì£¼ì„±ë¶„ ë¶„ì„]

---

> ì´ ë‚´ìš©ì€ ê³ ë ¤ëŒ€í•™êµ ê°•í•„ì„± êµìˆ˜ë‹˜ì˜ Business Analytics ìˆ˜ì—…ì„ ë³´ê³  ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.
>
> ì•„ë˜ ì´ë¯¸ì§€ í´ë¦­ ì‹œ ê°•ì˜ ì˜ìƒ Youtube URLë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
>
>[![01-4: Dimensionality Reduction - PCA](https://img.youtube.com/vi/bEX6WPMiLvo/mqdefault.jpg)](https://youtu.be/bEX6WPMiLvo)

## Principal Component Analysis: PCA

- ì›ë°ì´í„°ì˜ ë¶„ì‚°ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ëŠ” ì§êµí•˜ëŠ” ê¸°ì €ë¥¼ ì°¾ëŠ”ê²ƒ
- ë°ì´í„°ê°€ ê°€ì§€ê³  ìˆëŠ” íŠ¹ì •í•œ ì†ì„±ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ìƒˆë¡œìš´ ì„±ë¶„ì„ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒ

<img src='/img/pca02.png' width='600'>

### ëª©ì 

- ê¸°ì¤€ì— ë”°ë¼ ì˜ˆì¸¡ í›„ ìµœëŒ€í•œ ë¶„ì‚°ì„ ë³´ì¡´ í•  ìˆ˜ìˆëŠ” ê¸°ì¤€ ì§‘í•©ì„ ì°¾ëŠ”ë‹¤.
  - $$X_1, X_2, \cdots, X_p$$: Original variables
  - $$\mathbf{a}_i=[a_{i1}, a_{i2}, \cdots, a_{ip}]$$: $$i^{th}$$ basis or principal component
  - $$Y_1, Y_2, \cdots, Y_p$$: variables after the projection onto the $$i^{th}$$ basis

$$Y_1=\mathbf{a}_1'\mathbf{X}=a_{11}X_1+a_{12}X_2+\cdots+a_{1p}X_p$$

$$Y_2=\mathbf{a}_2'\mathbf{X}=a_{21}X_1+a_{22}X_2+\cdots+a_{2p}X_p$$

$$\vdots$$

$$Y_p=\mathbf{a}_p'\mathbf{X}=a_{p1}X_1+a_{p2}X_2+\cdots+a_{pp}X_p$$

<img src='/img/pca03.png' width='600'>

### Covariance

- $$\mathbf{X}$$: a data set (d by n, d: # of variables, n: # of records, ì—¬ê¸°ì„œ ë²¡í„°ëŠ” column-wise vector)

$$Cov(\mathbf{X})=\frac{1}{n}(\mathbf{X}-\bar{\mathbf{X}})(\mathbf{X}-\bar{\mathbf{X}})^T$$

- $$Cov(\mathbf{X})_{ij}=Cov(\mathbf{X})_{ji}$$
- Total variance of the data set: $$tr[Cov(\mathbf{X})]=Cov(\mathbf{X})_{11}+Cov(\mathbf{X})_{22}+\cdots+Cov(\mathbf{X})_{dd}$$

<img src='/img/pca04.png' width='600'>

### Projection onto a basis

<img src='/img/pca05.png' width='600'>

$$(\vec{b}-p\vec{a})^T\vec{a}=0 \; \Rightarrow \; \vec{b}^T\vec{a}-p\vec{a}^T\vec{a}=0 \; \Rightarrow \; p=\frac{\vec{b}^T\vec{a}}{\vec{a}^T\vec{a}}$$

$$\vec{x}=p\vec{a}=\frac{\vec{b}^T\vec{a}}{\vec{a}^T\vec{a}}\vec{a}$$

$$\text{If } \vec{a} \text{ is unit vector}$$

$$p=\vec{b}^T\vec{a} \; \Rightarrow \; \vec{x}=p\vec{a}=(\vec{b}^T\vec{a})\vec{a}$$

$$p$$ëŠ” ì‚¬ì˜ í›„ ê°’, $$\vec{b}$$ëŠ” $$x$$ëŠ” pc

### Eigenvalueì™€ Eigenvector

- í–‰ë ¬ Aê°€ ì£¼ì–´ì§€ë©´ ë‹¤ìŒ ë°©ì •ì‹ì„ ë§Œì¡±í•˜ëŠ” ìŠ¤ì¹¼ë¼ ê°’ ğœ†ê³¼ ë²¡í„° xë¥¼ ê°ê° Eigenvalue(ê³ ìœ  ê°’)ê³¼ Eigenvector(ê³ ìœ  ë²¡í„°)ë¼ê³  í•œë‹¤.

$$\mathbf{Ax}=\lambda\mathbf{x} \; \rightarrow \; \mathbf{(A-\lambda I)x}=0$$

- í–‰ë ¬ì„ ë²¡í„°ì— ê³±í•˜ë©´ ì„ í˜• ë³€í™˜ì´ ìˆ˜í–‰ëœë‹¤.
- ê³ ìœ  ë²¡í„°ëŠ” ë³€í™˜ì— ì˜í•´ ë°©í–¥ì„ ë³€ê²½í•˜ì§€ ì•ŠëŠ”ë‹¤.

<img src='/img/pca06.png' width='600'>

- í¬ê¸°ë§Œ ë°”ë€Œê³  ë°©í–¥ì´ ë°”ë€Œì§€ ì•ŠëŠ” ë²¡í„° -> Eigenvector
- í¬ê¸°ê°€ ë°”ë€ê²½ìš° ë³€í™”ëœ ì–‘ -> Eigenvalue
- í–‰ë ¬ Aê°€ non-singular d x d í–‰ë ¬ì¸ ê²½ìš°(ì—­í–‰ë ¬ì´ ì¡´ì¬)
  - d ê°œì˜ Eigenvalue-Eigenvector ìŒì´ ìˆë‹¤.
  - Eigenvector(ê³ ìœ  ë²¡í„°)ëŠ” ì„œë¡œ ì§êµ

$$\mathbf{tr(A)=\lambda_1+\lambda_2+\cdots+\lambda_d}$$

-----------------------------------------------

### Step 1: Data Centering

- ë³€ìˆ˜ì˜ í‰ê· ì„ 0ìœ¼ë¡œ ë§Œë“ ë‹¤.

<img src='/img/pca07.png' width='600'>

### Step 2: Formulate the optimization problem

- ë²¡í„° xê°€ ê¸°ì € wì— íˆ¬ì˜ í›„ ë¶„ì‚°

$$V=\frac{1}{n}(\mathbf{w^TX})(\mathbf{w^TX})^T=\frac{1}{n}\mathbf{w^TXX^Tw}=\mathbf{w^TSw}$$

- $$\mathbf{S}$$ëŠ” $$\mathbf{x}$$ê°€ ì •ê·œí™”ë˜ëŠ” í‘œë³¸ ê³µë¶„ì‚° í–‰ë ¬

- PCAì˜ ëª©ì ì€ íˆ¬ì˜ í›„ ë¶„ì‚° Vë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒ

$$\begin{aligned}\text{max}\,\mathbf{w^TSw} \\\text{s.t.}\,\mathbf{w^Tw}=1\end{aligned}$$

$$S=\begin{pmatrix}
0.6166 & 0.6154\\
0.6154 & 0.7166
\end{pmatrix}$$

### Step 3: Obtain the solution

- ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•(Lagrangian multiple)ì„ ì‚¬ìš©í•˜ë©´

$$L=\mathbf{w^TSw}-\lambda(\mathbf{w^Tw}-1)$$

$$\frac{\partial L}{\partial \mathbf{w}}=0 \; \Rightarrow \; \mathbf{Sw}-\lambda\mathbf{w}=0 \; \Rightarrow \; (\mathbf{S}-\lambda\mathbf{I})\mathbf{w}=0$$

$$Eigenvectors(\mathbf{w})=\begin{bmatrix}
0.6779 & -0.7352\\
0.7352 & 0.6779
\end{bmatrix} \;\;\;\;\;\; Eigenvalues(\lambda)=(1.2840 \;\; 0.0491)$$

### Step 4: Find the base set of bases

- ê³ ìœ  ê°’ì˜ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ê³ ìœ  ë²¡í„° ì •ë ¬
- $$\mathbf{w}_1$$ì„ ê³ ìœ  ë²¡í„° ì¤‘ í•˜ë‚˜ì´ê³  $$\lambda_1$$ì„ í•´ë‹¹ ê³ ìœ  ê°’ìœ¼ë¡œ ì§€ì •í•œë‹¤.
- ğ°1ì— íˆ¬ì˜ ëœ ìƒ˜í”Œì˜ ë³€í˜•ì€

$$\mathbf{v}=(\mathbf{w}_1^T\mathbf{X})(\mathbf{w}_1^T\mathbf{X})^T=\mathbf{w}_1^T\mathbf{XX}^T\mathbf{w}_1=\mathbf{w}_1^T\mathbf{S}\mathbf{w}_1$$

$$\text{Since} \; \mathbf{Sw}_1=\lambda_1\mathbf{w}_1, \;\;\mathbf{w}_1^T\mathbf{S}\mathbf{w}_1=\mathbf{w}_1^T\lambda\mathbf{w}_1=\lambda_1\mathbf{w}_1^T\mathbf{w}_1=\lambda_1$$

- ì´ ì˜ˆì—ì„œ í•˜ë‚˜ì˜ ê¸°ì¤€ìœ¼ë¡œ ì›ë˜ ë¶„ì‚°ì˜ 96%ë¥¼ ë³´ì¡´ í•  ìˆ˜ ìˆë‹¤.

$$\frac{\lambda_1}{\lambda_1+\lambda_2}=\frac{1.2840}{0.0491+1.2840}\fallingdotseq0.96$$

### Step 5: Extract new features

- ì„ íƒí•œ ë² ì´ìŠ¤ì— ì›ë³¸ ë°ì´í„°ë¥¼ íˆ¬ì˜

<img src='/img/pca08.png' width='600'>

### Step 6: Reconstruct the original data

- íˆ¬ì˜ëœ ê³µê°„ì˜ ë°ì´í„°ë¥¼ ì›ë˜ ê³µê°„ìœ¼ë¡œ ì¬êµ¬ì„±ì´ ê°€ëŠ¥í•˜ë‹¤.

<img src='/img/pca09.png' width='600'>

-----------------------------------------------------

### ìµœì ì˜ ì£¼ì„±ë¶„ì€ ëª‡ ê°œì¸ê°€?

- ëª…ì‹œì ì¸ í•´ê²°ì±… ì—†ìŒ
- ë¶„ì‚° ë³´ì¡´ë¥  ë° ë„ë©”ì¸ ì „ë¬¸ê°€ì˜ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ê²°ì • ê°€ëŠ¥
- ê³µë¶„ì‚°, ê³ ìœ ê°’ ê³ ìœ ë²¡í„°ì˜ ì†ì„±ì— ë”°ë¼
  - ë°ì´í„° ì„¸íŠ¸ì˜ ì´ ë³€ìˆ˜ = í‘œë³¸ ê³µë¶„ì‚° í–‰ë ¬ì˜ ê³ ìœ  ê°’ í•©ê³„

$$\begin{aligned}Total\; population\; variance= \sigma_{11}+\sigma_{22}+\cdots+\sigma_{dd}\\ =\lambda_1+\lambda_2+\cdots+\lambda_d\;\;\;\,\end{aligned}$$

- k ë²ˆì§¸ ê¸°ì¤€ì— ë°ì´í„°ë¥¼ íˆ¬ì˜í•˜ë©´ ë³´ì¡´ë˜ëŠ” ë¶„ì‚°ì˜ ì–‘ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$\frac{\lambda_k}{\lambda_1+\lambda_2+\cdots+\lambda_d}$$

- Scree í”Œë¡¯ì€ ì¼ë°˜ì ìœ¼ë¡œ PC ìˆ˜ë¥¼ ê²°ì •í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤.
  - x ì¶• : PC ìƒ‰ì¸
  - y ì¶• : í•´ë‹¹ ê³ ìœ  ê°’
- Scree í”Œë¡¯ì€ ì´ˆê¸° ë‹¨ê³„ì—ì„œ ë¹ ë¥´ê²Œ ê°ì†Œí•œë‹¤.
- ì¼ë¶€ í¬ì¸íŠ¸ ì´ìƒìœ¼ë¡œ ê°ì†Œí•˜ì§€ ì•ŠëŠ”ë‹¤.
- ì„ íƒ ë°©ë²• 1 : ì—˜ë³´ í¬ì¸íŠ¸ ì°¾ê¸°
- ì„ íƒ ë°©ë²• 2 : ë¯¸ë¦¬ ê²°ì •ëœ í•„ìˆ˜ ë¶„ì‚°(80%)ì„ ìœ ì§€í•˜ëŠ” ê°€ì¥ ì‘ì€ PC ì°¾ê¸°

<img src='/img/pca10.png' width='300'><img src='/img/pca11.png' width='300'>

-------------------------------------------

### PCA ì˜ˆì œ

- ì› ë°ì´í„°

<img src='/img/pca12.png' width='600'>

- ê° ì£¼ì„±ë¶„ì— ëŒ€í•œ ê³ ìœ  ë²¡í„° ê³ ìœ  ê°’
  - ì „ì²´ ë¶„ì‚°ì˜ 80 % ì´ìƒì„ ìœ ì§€í•˜ë ¤ë©´ ìµœì†Œ 5 PCê°€ í•„ìš”í•˜ë‹¤.

<img src='/img/pca13.png' width='600'>

- 2ì°¨ì›ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ê²°ê³¼

<img src='/img/pca14.png' width='600'>

- ì–¼êµ´ ì´ë¯¸ì§€ ë°ì´í„°

<img src='/img/pca15.png' width='600'>

<img src='/img/pca16.png' width='600'>

--------------------------------

### PCA í•œê³„

- non-Gaussian ë˜ëŠ” multimodal Gaussian ë¶„í¬ì—ì„œëŠ” ì˜ ì‘ë™í•˜ì§€ ì•ŠëŠ”ë‹¤.

<img src='/img/pca17.png' width='600'>

- ë¶„ë¥˜ì™€ëŠ” ì í•©í•˜ì§€ ì•Šë‹¤.
  - ë¶„ë¥˜ì—ëŠ” (F)LDA / Linear Discriminant Analysis ë°©ë²•ì´ ìˆìŒ
<img src='/img/pca18.png' width='600'>
