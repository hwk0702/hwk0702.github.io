---

layout: post

title: "KFDA"

date: 2021-08-20 21:13:07

categories: [ML/DL/Machine Learning]

description:

image: /img/KFDA.png

published: True

canonical_url:

tags: [Machine Learning, Kernel-based Learning, KFDA, Kernel Fisher Discriminant Analysis]

---

> 이 내용은 고려대학교 강필성 교수님의 Business Analytics 수업을 보고 정리한 것입니다.
>
> 아래 이미지 클릭 시 강의 영상 Youtube URL로 넘어갑니다.
>
>[![02-2: Kernel-based Learning - SVM (Linear Case with Hard Margin)](https://img.youtube.com/vi/XpkOcsGTS8k/mqdefault.jpg)](https://www.youtube.com/watch?v=XpkOcsGTS8k&list=PLetSlH8YjIfWMdw9AuLR5ybkVvGcoG2EW&index=12)

# Kernel Fisher Discriminant Analysis(KFDA)

LDA의 Kernel화된 확장판이라고 보면 됨

## 1. Linear Discriminant Analysis

data의 class가 잘 구분이 되게 사영시키기 위한 기저 찾기

### ✓ Two type of class distance

- Between-class distance : 두 class의 centroid간의 거리 (커야하고)
- Within-class distance : 각각의 객체로 부터 해당 class의 centroid까지의 누적 또는 평균 거리 (작아야 한다.)

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled.png' width='600'>

### ✓ Fisher's LDA

- D차원 입력 벡터 x를 가져와 하나의 차원으로 투영

$$y=\textbf{w}^T\textbf{x}$$

- $$\textbf{m}_1$$은 한 class의 centroid, $$\textbf{m}_2$$를 다른 class의 centroid라고 하면,

$$\textbf{m}_1=\frac{1}{N_1}\sum_{n\in C_1}\textbf{x}_n,\;\;\textbf{m}_2=\frac{1}{N_2}\sum_{n\in C_2}\textbf{x}_n$$

- Objective 1: between class variance가 최대한 커지는 $$\textbf{w}$$

$$m_2-m_1=\textbf{w}^T(\textbf{m}_2-\textbf{m}_1), \;\; m_k=\textbf{w}^T\textbf{m}_k$$

- Objective 2: Within-class variance를 최소화하는 $$\textbf{w}$$

$$s_k^2=\sum_{n\in C_k}(y_n-m_k)^2$$

- Fishers' criterion

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 1.png' width='600'>

$$J(\mathbf{w})$$를 maximize하는 것이 목표!

- Find $$\mathbf{w}$$

$$J(\textbf{w})=\frac{\textbf{w}^T\textbf{S}_B\textbf{w}}{\textbf{w}^T\textbf{S}_W\textbf{w}}$$

를 미분한 값이 0이 되어야 한다.

$$\frac{\partial J(\textbf{w})}{\partial \textbf{w}}=0$$

분모만 가지고 오면

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 2.png' width='400'>

- $$\textbf{S}_B$$는 항상 $$(\textbf{m}_2-\textbf{m}_1)$$의 방향
- $$\frac{\textbf{w}^T\textbf{S}_B\textbf{w}}{\textbf{w}^T\textbf{S}_W\textbf{w}}$$를 scalar
- 따라서 Fisher's linear discriminant

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 3.png' width='200'>

## 2. Kernel Fisher Discriminant (KFD)로 확장

- 데이터를 고차원 벡터로 mapping 후에 covariance matrix 계산 ($$\textbf{m}^\Phi$$는 sentroid)

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 4.png' width='600'>

- within-class variance, between-class variance

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 5.png' width='600'>

### ✓ Objective functions

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 6.png' width='200'>

### ✓ Projected vector

Feature space 내의 객체들의 선형 결합으로 표현 될 수 있다.

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 7.png' width='300'>

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 8.png' width='300'>

### ✓ Projected mean

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 9.png' width='600'>

### ✓ Objected function (Numerator)

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 10.png' width='500'>

### ✓ Objected function (Denominator)

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 11.png' width='600'>

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 12.png' width='600'>

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 13.png' width='600'>

$$\alpha$$와 커널 함수 매트릭스로 나온다는 것이 핵심, 결국 feature space내에서 실질적인 계산을 하지 않고 입력 공간네에서 커널 함수만 정의가 되면 분모도 계산할 수 있다.

### ✓ Objective function ($$\alpha$$)

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 14.png' width='150'>

### ✓ 1차 도함수가 0이 되게

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 15.png' width='200'>

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 16.png' width='300'>

### ✓ 새 데이터 포인트의 투영 값

<img src='/img/Kernel Fisher Discriminant Analysis(KFDA) e49229e73987402eb4d7ef75c0b15226/Untitled 17.png' width='300'>
