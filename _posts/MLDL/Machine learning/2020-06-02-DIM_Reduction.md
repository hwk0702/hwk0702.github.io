---

layout: post

title: "차원 축소"

date: 2020-06-02 16:02:07

categories: [ML/DL/Machine Learning]

description:

image: /img/dim_red.png

published: true

canonical_url:

---

> 이 내용은 핸즈온 머신러닝 2판 책을 보고 정리한 것 입니다.
>
> <img src='http://image.yes24.com/goods/89959711/800x0' width='150'>

차원 축소(Dimensionality Reduction)
----------------------------------------------

- PCA, 커널 PCA, LLE 등

|장점|단점|
|---|----|
|* 특성 수를 줄여 훈련 속도를 높일 수 있다. <br> * 잡음이나 불필요한 세부사항을 걸러내 성능을 높일 수 있다. <br> * 데이터 시각화에도 유용하다. |* 일부 정보가 유실된다.(성능이 조금 나빠질 수 있다.) <br> * 작업 파이프라인이 더 복잡하게 되고 유지 관리가 어려워진다. |
  
---

#### 1. 차원의 저주

-	**차원의 저주**: 많은 특성은 훈련을 느리게 하고, 좋은 솔루션을 찾기 어렵게 한다.
- 고차원 초입방체에 있는 대다수의 점은 경계와 매우 가까이 있다.
- 고차원으로 갈수록 데이터간의 거리가 멀어진다. 따라서 예측을 위해 춸씬 많은 외삽을 해야하기 때문에 저차원일 때보다 예측이 더 불안정하다. (과대적합 위험이 커짐)
- 해결책으로는 훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트의 크기를 키우는 것(일정 밀도에 도달하기 위해 필요한 샘플 수는 차원 수가 커짐에 따라 기하급수적 증가)

<img src='/img/dim_red1.png' width='400'>

---

#### 2. 차원 축소를 위한 접근 방법

- 투영, 매니폴드 학습

##### 2.1 투영

- 많은 특성은 거의 변화가 없는 반면, 다른 특성들은 서로 강하게 연관되어 있다.
- 모든 훈련 샘플이 **고차원 공간 안의 저차원 부분 공간**에 놓여 있다.

<img src='/img/dim_red2.png' width='400'>

- 모든 훈련 샘플을 저차원 부분 공간에 수직으로 투영하면 차원이 축소된 데이터 셋을 얻을 수 있다.

<img src='/img/dim_red3.png' width='400'>

- 스위스 롤 데이터셋처럼 부분 공간이 뒤틀리거나 휘어 있는 경우 투영이 최선의 방법은 아니다.

<img src='/img/dim_red4.png' width='400'>
<img src='/img/dim_red5.png' width='400'>

##### 2.2 매니폴드 학습

- 스위스 롤은 2D 매니폴드의 한 예
- d차원 매니폴드: d차원 초평면으로 보일 수 있는 n차원 공간의 일부(d<n)
- 매니폴드 학습: 실제 고차원 데이터셋이 더 낮은 저차원 매니폴드에 가깝게 놓여 있다는 **매니폴드 가정** 또는 **매니폴드 가설**에 근거하여 매니폴드를 모델링 하는 방법
- 매니폴드 가정은 처리해야 할 작업이 저차원의 매니폴드 공간에 표현되면 더 간단해질 것이라는 가정과 병행. 이 가정이 항상 유효하지는 않음

<img src='/img/dim_red6.png' width='400'>

---

#### 3. 주성분 분석(PCA)

- 데이터에 가장 가까운 초평면을 정의한 다음, **데이터를 초평면에 투영시키는 방법**

##### 3.1 분산 보존

- 초평면에 훈련 세트를 투영하기 전에 올바른 초평면을 선택해야 한다.
- 올바른 초평면: 다른 방향으로 투영하는 것보다 **분산이 최대로 보존되는 축**을 선택하는 것이 정보가 가장 적게 손실되므로 합리적(원본 데이터셋과 투영된 것 사이의 평균 제곱 거리를 최소화)

<img src='/img/dim_red7.png' width='400'>

##### 3.2 주성분

- 고차원 데이터셋을 분산이 최대인 축을 계속 찾아가며 축소하는 경우 **i번째 축을 이 데이터의 i번째 주성분(PC)**라고 한다.

---

아직 작성 중...
추가 예정 ...