---

layout: post

title: "인공 신경망"

date: 2020-07-09 11:27:07

categories: [ML/DL/Deep Learning]

description:

image: /img/ann.png

published: true

canonical_url:

tags: [인공 신경망, ANN, Artificial Neural Network, 뉴런, Neuron, 시냅스, 퍼셉트론, Perceptron, 딥러닝, Deep Learning, 텐서플로우, Tensorflow, 케라스, Keras]

---

> 이 내용은 핸즈온 머신러닝 2판 책을 보고 정리한 것 입니다.
>
> <img src='http://image.yes24.com/goods/89959711/800x0' width='150'>

## 인공 신경망 (Artificial Neural Network, ANN)

- 인공 신경망은 뇌에 있는 생물학적 뉴런의 네트워크에서 영감을 받은 머신러닝 모델
- 딥러닝의 핵심
- 다재다능하고 강력하고 확장성이 좋다

<img src='/img/ann1.png' width='500'>

인공 신경망이 다시 각광받게 된 이유

- 데이터가 많아졌다.(규모가 크고 복잡한 문제에서 다른 머신러닝 기법보다 좋은 성능)
- 컴퓨터 하드웨어가 크게 발전(GPU, 클라우드 플랫폼)
- 훈련 알고리즘이 향상
- 일부 인공 신경망의 이론상 제한이 실전에서는 문제가 되지 않는다.
- 투자와 진보의 선순환

---

#### 1. 생물학적 뉴런

- 세포는 핵을 포함하는 세포체와 복잡한 구성 요소
- 수상돌기라는 나뭇가지 모양의 돌기와 축삭돌기라는 아주 긴 돌기 하나
- 축삭돌기의 끝은 축삭끝가지라는 여러 가지로 나뉘고, 이 가지 끝은 시냅스 말단(또는 시냅스)이라는 미세한 구조로 다른 뉴런의 수상돌기나 세포체에 연결
- 뉴런은 활동 전위 또는 간단히 신호라고 부르는 짧은 전기 자극을 만들고, 이 신호는 축삭돌기를 따라 이동하여 시냅스가 신경 전달물질이라는 화학적 신호를 발생하게 함

<img src='/img/ann2.png' width='500'>

#### 2. 퍼셉트론

##### 2.1 뉴런을 사용한 논리 연산

- 생물학적 뉴런에서 착안한 매우 단순한 신경망 모델 인공 뉴런
- 하나 이상의 이진(on/off) 입력과 이진 출력 하나
- 입력이 일정 개수만큼 활성화되었을 때 출력을 내보낸다.

<img src='/img/ann3.png' width='500'>

##### 2.2 퍼셉트론

- 입력변수의 값들에 대한 가중합에 대한 활성함수를 적용하여 최종 결과물 생성
- TLU(threshold logic unit) 또는 LTU(linear threshold unit)이라고 불림

<img src='/img/ann4.png' width='400'>

- 퍼셉트론은 층이 하나뿐인 TLU로 구성
- 한 층에 있는 모든 뉴런이 이전 층의 모든 뉴런과 연결되어 있을 때 이를 **완전 연결층(fully connected layer)** 또는 **밀집 층(dense layer)** 라고 부름

<img src='/img/ann5.png' width='400'>

완전 연결 층의 출력 계산

$$h_{W,b}(X)=\phi(XW+b)$$

- 헤브의 규칙에 따르면 한 번에 한 개의 샘플이 주입되면 각 샘플에 대해 예측을 만들고 잘못된 예측을 하는 모든 출력 뉴런에 대해 입력에 연결된 가중치를 강화

퍼셉트론 학습 규칙(가중치 업데이트)

$${w_{i,j}}^{(next\:step)}=w_{i,j}+\eta(y_j-\hat{y}_j)x_j$$

> - $${w_{i,j}}$$는 $$i$$번째 입력 뉴런과 $$j$$번째 출력 뉴런 사이를 연결하는 가중치
> - $$x_i$$는 현재 훈련 샘플의 $$i$$번째 뉴런의 입력값
> - $$\hat{y}_j$$는 현재 훈련 샘플의 $$j$$번째 출력 뉴런의 출력값
> - $$y_j$$는 현재 훈련 샘플의 $$j$$번째 출력 뉴런의 타깃값
> - $$\eta$$는 학습률

##### 2.3 퍼셉트론의 구조

<img src='/img/ann6.png' width='400'>

1) **입력 노드(input node)**: 우리가 알고 있는 개별 설명 변수가 각각의 입력 노드에 해당

2) **은닉 노드(hidden node)**: 개별 설명변수들의 값을 취합(선형 결합)하여 비선형 변환(활성화)을 수행

- 활성화 함수의 역할: 각 노드가 이전 노드들로부터 전달받은 정보를 다음 노드에 얼마만큼 전달해 줄 것인가를 결정
- 대표적인 활성화 함수

|활성화 함수|설명|
|---|---|
|Sigmoid|가장 일반적으로 사용되는 활성화 함수, [0, 1]의 범위를 가지며 학습 속도가 상대적으로 느림|
|Tanh|활성화함수와 형태는 유사하나 [-1, 1]의 범위를 가져 학습 속도가 상대적으로 빠름|
|ReLu|학습속도가 매우 빠르며 상대적으로 계산이 쉬움(지수함수 형태를 사용하지 않는다)|

<img src='/img/ann7.png' width='600'>

- 활성화 함수의 자세한 내용은 다음에 다룰 예정

3) **출력 노드(output node)**: 은닉 노드에서 생성된 값을 그대로 받아들임(다층 퍼셉트론에서는 여러 은닉 노드에서 정보 취합)

4) **목적함수**

- 퍼셉트론의 목적: 주어진 학습 데이터의 입력 정보와 출력 정보의 관계를 잘 찾도록 가중치를 조절
- 현재 퍼셉트론의 결과물이 실제 정답에 얼마나 가까운지를 손실 함수(loss function)를 통해 측정
- 전체 데이터 셋에 대해서 현재 퍼셉트론이 얼마나 잘못하고 있는지는 비용 함수(cost function)을 사용

---

경사 하강법 (Gradient Descent) 관련 내용은 Machine learning 카테고리에서 [경사하강법](https://hwk0702.github.io/ml/dl/machine%20learning/2020/05/28/GD/) 내용 참고
