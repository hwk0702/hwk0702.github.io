---

layout: post

title: "USAD : UnSupervised Anomaly Detection on Multivariate Time Series"

date: 2021-02-15 19:30:07

categories: [Paper Review]

description:

image: /img/USAD.png

published: True

canonical_url:

tags: [Anomaly Detection, Unsupervised, Time series, USAD, UnSupervised Anomaly Detection on Multivariate Time Series]

---

> USAD : UnSupervised Anomaly Detection on Multivariate Time Series Julien Audibert, Pietro Michiardi, Frédéric Guyard, Sébastien Marti, Maria A. Zuluaga, KDD 2020
>
> 논문을 읽고 고려대학교 산업경영공학부 DSBA 연구실 최희정 선배님의 세미나 발표 자료를 참고하였습니다.
>
> [![[Paper Review] USAD: UnSupervised Anomaly Detection on Multivariate Time Series](https://img.youtube.com/vi/gCleQ9JxibI/mqdefault.jpg)](https://youtu.be/gCleQ9JxibI)

USAD : UnSupervised Anomaly Detection on Multivariate Time Series
-----------------------------------------------------------------

-	차별점 : 결과가 좋은 여러 기법들이 존재하지만, 수행 기준에서 training time(i.e. energy consumption)을 고려하지 않는다.

### Problem formulation

-	Dataset : set of multivariate time series

$$\mathscr{T}=\left \{ \textbf{x}_1, \dots, \textbf{x}_T \right \}, \;\;\textbf{x}\in \mathbb{R}^m$$

-	window : 길이가 k인 time window

$$W*t=\left \{ \textbf{x}*{t-K+1}, \dots, \textbf{x}_{t-1}, \textbf{x}_t \right \}$$

-	time window를 input으로 t시점의 normal/abnormal 여부를 예측

<img src='/img/usad1.png' width='600'>

---

### Unsupervised Anomaly Detection

#### AE-based multivariate time series AD

-	Autoencoder 구조인 Encoder $$E$$와 Decoder $$D$$를 이용
-	original input vector $$X$$와 reconstruction $$R$$의 차이인 reconstruction error를 최소화
-	정상데이터의 reconstruction error를 기반으로 학습하여 정상 데이터의 분포를 학습함
-	reconstruction error를 anomaly scores로 사용, threshold를 초과하면 이상치로 탐지함

$$\begin{matrix}\text{as:} & \\ & \;\;\;\;\;\;\; \mathscr{L}_{AE}=\left | X-AE(X) \right |_2,\\\text{where} & \\ & \;\;\;\;\;\;\; AE(X)=D(Z), \;\;\; Z=E(X)\\\end{matrix}$$

$$\text{and } \left | \cdot \right |_2 \text{ denotes the L2-norm.}$$

<img src='/img/usad3.png' width='600'>

-	단점: anomaly가 매우 작은 경우, normal data와 근접하는 경우에 reconstruction error가 매우 작아 detection이 잘 안됨.

#### GAN-based multivariate time series AD

-	GAN을 이용하여 anomaly detection 하는 방법
-	generator와 discriminator 두 개의 모델을 이용하여 generator는 실제와 비슷하게 samples를 생성하여 discriminator를 속이고 discriminator는 generator가 만든 가짜를 찾아내는 방법
-	GAN-AD에 대해서는 저번 포스트에서 자세하게 다뤘으니 참고할 것 [포스트 보러가기](https://hwk0702.github.io/treatise%20review/2021/02/08/GAN_AD/)

<img src='/img/usad4.png' width='600'>

-	학습이 불안정하여 mode collapse, non-convergence 문제가 존재

---

### Unsupervised Anomaly Detection(USAD)

-	두 단계의 Adversarial training framework 내의 AE 아키텍처 이용
-	세가지 elements로 구성
	-	encoder network $$E$$, two decoder networks $$D_1, \;D_2$$

$$AE_1(W)=D_1(E(W)), \;\; AE_2(W)=D_2(E(W))$$

1) 두 AE가 normal input windows W를 재구축하게 학습 2) 두 AE가 adversarial하게 학습, $$AE_1$$은 $$AE_2$$를 속이게 학습하고, $$AE_2$$는 실제 데이터와 $$AE_1$$으로부터 재구축된 데이터 구별하게 학습

<img src='/img/usad2.png' width='600'>

#### Phase 1. Autoencoder training

-	Input data W를 encoder E를 이용하여 latent space Z로 압축, 각 decoder로 재구축하게 학
-	이때, adversarial training을 위해 Encoder ��, Decoder ����, Decoder ����를 기반으로 encoder를 공유하는 두 개의 AE를 구축함

$$\begin{matrix}\mathscr{L}*{AE_1} = \left | W-AE_1(W) \right |_2 \\\mathscr{L}*{AE_2} = \left | W-AE_2(W) \right |_2 \\\end{matrix}$$

<img src='/img/usad6.png' width='600'>

#### Phase 2. Adverserial training

-	$$AE_2$$는 실제 데이터와 $$AE_1$$이 만든 데이터 구별하게 학습
-	$$AE_1$$는 $$AE_2$$를 속이게 학습

$$\underset{AE_1}{\text{min}}\underset{AE_2}{\text{max}}\left | W-AE_2(AE_1(W)) \right |_2$$

<img src='/img/usad7.png' width='600'>

#### Two-phase training

-	USAD를 구성하는 두 AE의 학습목표는 다음과 같음
	-	$$AE_1$$: input을 잘 복원하면서 $$AE_2$$를 잘 속이는 모델 학습
	-	$$AE_2$$: input을 잘 복원하면서 $$AE_1$$이 복원한 데이터와 input을 잘 구별하는 모델 학습
-	$$AE_2$$ 도입 및 adversarial training을 통해 정상 데이터와 유사한 이상치를 탐지하는 것을 가능하게 하며, AE 구조를 통해 안정적인 학습이 가능하게 함

$$\begin{matrix}\mathscr{L}*{AE_1}=\frac{1}{n}\left | W-AE_1(W) \right |_2 + (1-\frac{1}{n})\left | W-AE_2(AE_1(W)) \right |_2 \\\mathscr{L}*{AE_2}=\frac{1}{n}\left | W-AE_2(W) \right |_2 - (1-\frac{1}{n})\left | W-AE_2(AE_1(W)) \right |_2\end{matrix}$$

-	n은 epoch
-	안정적인 학습을 위해 초반에는 reconstruction에 가중치를 주고, 후반에는 adversarial training에 가중치 부여

#### Inference

-	anomaly score
	-	학습이 완료된 두 AE를 기반으로 아래와 같이 anomaly score를 산출함 $$\mathscr{A}(\hat{W})=\alpha\left | \hat{W} - AE_1(\hat{W}) \right |_2+\beta\left | \hat{W} - AE_2(AE_1(\hat{W})) \right |_2$$
	-	$$\alpha+\beta=1$$

<img src='/img/usad8.png' width='600'>

-	계수의 비중에 따라 아래와 같이 false positive와 true positive 간의 trade-off가 발생함
	-	�� > ��: true & false positive 감소 → low detection sensitivity scenario
	-	�� < ��: true & false positive 증가 → high detection sensitivity scenario

|               |          | True                     | True                      |
|---------------|----------|--------------------------|---------------------------|
|               |          | Abnormal                 | Normal                    |
| **Predicted** | Abnormal | **<u>True Positive</u>** | **<u>False Positive</u>** |
| **Predicted** | Normal   | False Negative           | True Negative             |

---

### Experiments and results

#### Datasets

-	5개의 publicly available datasets과 1개의 internal dataset으로 결과를 냄
-	Window 내부의 한 시점이라도 abnormal이면 해당 구간의 ground truth를 abnormal로 부여하는 point-adjust 방식을 통해 모델의 성능을 평가함

<img src='/img/usad9.png' width='400'>

#### Evaluation Metrics

-	Precision($$P$$), Recall($$R$$), F1 score($$F1$$), average precision과 average recall을 이용하여 계산한 F1 score($$F1^*$$) 사용하여 평가

$$P=\frac{TP}{TP+FP}, \;\; R=\frac{TP}{TP+FN}, \;\; F1=2\cdot\frac{P\cdot R}{P+R}, \;\; F1^*=2\cdot\frac{\bar{P}\cdot \bar{R}}{\bar{P}+\bar{R}} $$

#### Baselines

-	5개의 baselines를 이용하여 비교

<img src='/img/usad10.png' width='400'>

#### Overall performance

-	제안한 방법론이 대부분의 데이터에서 우수한 성능을 도출함
-	시계열 정보를 사용하지 않은 IF와 DAGMM이 낮은 성능을 도출하는 것을 통해 temporal information을 모델링하는 것이 중요함을 알 수 있음

<img src='/img/usad11.png' width='600'>

#### Effect of parameters

-	SWaT dataset을 기반으로 총 4개의 parameter가 모델에 미치는 영향을 확인함

	-	Down-sampling: 데이터를 샘플링하는 주기
	-	Windows size: input의 sequence 길이
	-	Dimension of latent space: AE의 latent space 차원
	-	Percentage of anomalies: train dataset에 포함되는 anomaly 비율

-	Down-sampling값이 5일때 최고 성능을 도출하며,비율의 값에 관계 없이 일정한 성능을 도출하는 것을 통해 모델이 down-sampling에 민감하지 않음을 알 수 있음

-	Window size가 10일때 최고 성능을 도출하는 것을 통해 각 시점이 이상치 탐지에 큰 영향을 미치고 USAD가 이상치를 빠르게 탐지할 수 있다고 볼 수 있음

<img src='/img/usad12.png' width='600'>

-	Dimension of latent space가 20일 때 최고 성능을 도출하며, 값이 작을 때는 정보 손실이 발생하고 클 때는 memorization이 발생해 성능이 낮은 것을 확인할 수 있음
-	Training data의 percentage of anomalies가 증가함에 따라 precision이 크게 감소하는 것을 통해 false positive가 증가함을 알 수 있음

<img src='/img/usad13.png' width='600'>

-	anomalyscore의 parameter ��와 ��의 값에따라 상이한 결과가 도출되는 것을 통해 모델이 ��와 ��에 민감함을 알 수 있으며, ��가 증가하고 ��가 감소할수록 false positive와 true positive가 모두 감소하는 것을 확인할 수 있음

<img src='/img/usad16.png' width='250'>

#### Ablation Study

-	SMD, SMAP, MSL datasets을 기반으로 2개의 phase의 효과를 검증
-	Adversarial training의 접목이 AE의 성능을 유의미하게 향상시키는 것을 통해 adversarial training이 정상 데이터와 유사한 이상치를 탐지하는데 도움이 된다고 볼 수 있음

<img src='/img/usad14.png' width='500'>

#### Feasibility Study

-	실제현장에서 사용되는 internal dataset을 기반으로 제안한 모델의 feasibility를 검증한 결과 이상치 탐지 성능이 우수한것을 확인함

<img src='/img/usad16.png' width='250'>

<img src='/img/usad15.png' width='600'>
